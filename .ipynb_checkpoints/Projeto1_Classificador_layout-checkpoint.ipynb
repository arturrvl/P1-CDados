{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nome: Artur Reppucci Vaz de Lima\n",
    "\n",
    "##### Nome: Pedo Paulo Tibério"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador  automático de sentimento\n",
    "Estamos trabalhando com a maior empresa do mundo, Apple, então consideramos como relevante quaisquer opiniões diretas e indiretas a respeito de produtos, softwares e atualizações relacionadas à marca. Alguns comentários sarcásticos e irônicos foram classificados como relevantes, já que, os analisando bem, podemos indentificar alguma insatisfação do cliente. Entretanto, quando ocorrer a classificação automática, talvez o código não consiga identificar como relevante alguns tweets sarcásticos/irônicos, já que se trata de um sentimento puramente humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrei o arquivo Apple.xlsx, tudo certo para prosseguir com a prova!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\limaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import RSLPStemmer \n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "filename = 'Apple.xlsx'\n",
    "if filename in os.listdir():\n",
    "    print(f'Encontrei o arquivo {filename}, tudo certo para prosseguir com a prova!')\n",
    "else:\n",
    "    print(f'Não encontrei o arquivo {filename} aqui no diretório {os.getcwd()}, será que você não baixou o arquivo?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando a base de dados com os tweets classificados como relevantes e não relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.ExcelFile('Apple.xlsx')\n",
    "treino = pd.read_excel(excel, 'Treinamento')\n",
    "teste = pd.read_excel(excel, 'Teste')\n",
    "treino.Treinamento = treino.Treinamento.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando o banco de dados 'Treinamento'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função que faz uma \"limpeza'' na base de dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpeza(tweet):\n",
    "    link = re.sub(r'http\\S+', '', tweet)\n",
    "    mencao = re.sub(\"@[A-Za-z0-9_]+\",\"\", link)\n",
    "    point = '[!-.:?;@\\/]'\n",
    "    pad = re.compile(point)\n",
    "    text_subbed = re.sub(pad, ' ', mencao)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Código que divide dependendo da relevância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_rel = treino[treino.Classificação == 1]\n",
    "com_rel_treino = com_rel['Treinamento']\n",
    "com_rel_lista = com_rel_treino.values.tolist()\n",
    "\n",
    "com_irrel = treino[treino.Classificação == 0]\n",
    "com_irrel_treino = com_irrel['Treinamento']\n",
    "com_irrel_lista = com_irrel_treino.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Código que separa as palavras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faz essa parte pq n tenho ideia de como fazer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### função que remove palavras sem importância (stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removesp (lista):\n",
    "    sp = nltk.corpus.stopwords.words('portuguese')\n",
    "    palavras_safe = []\n",
    "    for i in lista:\n",
    "        if i not in sp:\n",
    "            palavras_safe.append(i)\n",
    "    return palavras_safe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### função que deixa apenas o radical da palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apenas_radical (lista):\n",
    "    radical = RSLPStemmer()\n",
    "    radical_palavras = []\n",
    "    for i in lista:\n",
    "        radical_palavras.append(radical.stem(i.lower()))\n",
    "    return radical_palavras\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_rel = apenas_radical(removesp(com_rel_pp))\n",
    "palavras_irrel = apenas_radical(removesp(com_irrel_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appl              0.067797\n",
      "pra               0.011689\n",
      "io                0.011105\n",
      "watch             0.009936\n",
      "iphon             0.009936\n",
      "                    ...   \n",
      "@jsmiranda12      0.000584\n",
      "ajud              0.000584\n",
      "comerc            0.000584\n",
      "@cmerig           0.000584\n",
      "@joopedrolins2    0.000584\n",
      "Length: 916, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Frequências relativas\n",
    "p_r_s = pd.Series(palavras_rel)\n",
    "p_r_r = p_r_s.value_counts(True)\n",
    "\n",
    "p_i_s = pd.Series(palavras_irrel)\n",
    "p_i_r = p_i_s.value_counts(True)\n",
    "\n",
    "\n",
    "total_palavras = palavras_rel + palavras_irrel\n",
    "t_p_s = pd.Series(total_palavras)\n",
    "t_p_r = t_p_s.value_counts(True)\n",
    "print(p_r_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificador(tweet):\n",
    "    tweet_p_p : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou entre emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
